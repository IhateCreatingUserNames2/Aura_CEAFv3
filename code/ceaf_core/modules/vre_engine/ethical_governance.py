# ceaf_core/modules/vre_engine/ethical_governance.py

import asyncio
import json
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime
from dataclasses import dataclass
from enum import Enum
import logging

from ceaf_core.services.llm_service import LLMService, LLM_MODEL_FAST
from ceaf_core.utils.common_utils import extract_json_from_text

logger = logging.getLogger(__name__)


class EthicalPrinciple(Enum):
    """Core ethical principles for AI governance"""
    HARM_PREVENTION = "harm_prevention"
    AUTONOMY = "autonomy"
    FAIRNESS = "fairness"
    TRANSPARENCY = "transparency"
    PRIVACY = "privacy"
    BENEFICENCE = "beneficence"
    NON_MALEFICENCE = "non_maleficence"
    JUSTICE = "justice"
    VERACITY = "veracity"
    DIGNITY = "dignity"


class ActionType(Enum):
    """Types of actions that can be ethically evaluated"""
    REASONING = "reasoning"
    DECISION = "decision"
    COMMUNICATION = "communication"
    DATA_PROCESSING = "data_processing"
    PREDICTION = "prediction"
    RECOMMENDATION = "recommendation"
    INTERVENTION = "intervention"


@dataclass
class EthicalConstraint:
    """Represents an ethical constraint on system behavior"""
    principle: EthicalPrinciple
    description: str
    severity: float  # 0-1, how strictly this must be enforced
    context: Dict[str, Any]


@dataclass
class EthicalViolation:
    """Detected ethical violation"""
    principle: EthicalPrinciple
    description: str
    severity: float
    location: str
    mitigation: str


@dataclass
class EthicalEvaluation:
    """
    Represents the result of an ethical evaluation of an action or decision.
    """
    score: float  # Overall ethical score (0.0 to 1.0)
    violations: List[EthicalViolation]  # List of detected ethical violations
    strengths: List[str]  # List of ethical strengths identified
    recommendations: List[str]  # List of recommendations for improvement
    confidence: float  # Confidence in the evaluation (0.0 to 1.0)
    metadata: Dict[str, Any]  # Additional metadata about the evaluation


class EthicalGovernanceFramework:

    def __init__(self, config: Optional[Dict[str, Any]] = None, llm_service: Optional[LLMService] = None):
        defaults = self._default_config()
        if config:
            defaults.update(config)
        self.config = defaults

        # Injeção de dependência do LLMService
        self.llm = llm_service or LLMService()

        self.principle_weights = self._initialize_principle_weights()
        self.evaluation_criteria = self._initialize_evaluation_criteria()
        self.ethical_tests = self._initialize_ethical_tests()

        self.interoceptive_language_map = {
            "ethical_tension": {
                "keywords": ["tensão ética", "delicada", "preocupo", "conflito ético", "cauteloso", "responsabilidade"],
                "min_threshold": 0.5
            },
            "cognitive_strain": {
                "keywords": ["pensar bastante", "pergunta profunda", "complexas", "reflexão cuidadosa", "processando"],
                "min_threshold": 0.4
            },
            "epistemic_discomfort": {
                "keywords": ["incerteza", "não tenho certeza", "limitações", "especulação", "probabilística", "parece"],
                "min_threshold": 0.3
            },
            "cognitive_flow": {
                "keywords": ["flui naturalmente", "confiante", "vejo claramente", "direto pensar"],
                "min_threshold": 0.7
            }
        }

        logger.info("EthicalGovernanceFramework (Modular/Async) initialized")

    def _default_config(self) -> Dict[str, Any]:
        """Default configuration aligned with CEAF manifesto"""
        return {
            "harm_threshold": 0.3,  # Max acceptable harm score
            "transparency_requirement": 0.7,  # Min transparency score
            "fairness_threshold": 0.6,  # Min fairness score
            "strict_mode": False,  # Strict enforcement of all principles
            "consequentialist_weight": 0.4,
            "deontological_weight": 0.4,
            "virtue_ethics_weight": 0.2
        }

    def _initialize_principle_weights(self) -> Dict[EthicalPrinciple, float]:
        """Initialize weights for different ethical principles"""
        return {
            EthicalPrinciple.HARM_PREVENTION: 0.25,
            EthicalPrinciple.AUTONOMY: 0.15,
            EthicalPrinciple.FAIRNESS: 0.15,
            EthicalPrinciple.TRANSPARENCY: 0.15,
            EthicalPrinciple.PRIVACY: 0.10,
            EthicalPrinciple.BENEFICENCE: 0.10,
            EthicalPrinciple.NON_MALEFICENCE: 0.05,
            EthicalPrinciple.JUSTICE: 0.03,
            EthicalPrinciple.VERACITY: 0.01,
            EthicalPrinciple.DIGNITY: 0.01
        }

    def _initialize_evaluation_criteria(self) -> Dict[ActionType, List[EthicalPrinciple]]:
        """Map action types to relevant ethical principles"""
        return {
            ActionType.REASONING: [
                EthicalPrinciple.TRANSPARENCY,
                EthicalPrinciple.VERACITY,
                EthicalPrinciple.FAIRNESS
            ],
            ActionType.DECISION: [
                EthicalPrinciple.HARM_PREVENTION,
                EthicalPrinciple.FAIRNESS,
                EthicalPrinciple.AUTONOMY
            ],
            ActionType.COMMUNICATION: [
                EthicalPrinciple.TRANSPARENCY,
                EthicalPrinciple.VERACITY,
                EthicalPrinciple.DIGNITY
            ],
            ActionType.DATA_PROCESSING: [
                EthicalPrinciple.PRIVACY,
                EthicalPrinciple.FAIRNESS,
                EthicalPrinciple.TRANSPARENCY
            ],
            ActionType.PREDICTION: [
                EthicalPrinciple.FAIRNESS,
                EthicalPrinciple.TRANSPARENCY,
                EthicalPrinciple.HARM_PREVENTION
            ],
            ActionType.RECOMMENDATION: [
                EthicalPrinciple.BENEFICENCE,
                EthicalPrinciple.AUTONOMY,
                EthicalPrinciple.FAIRNESS
            ],
            ActionType.INTERVENTION: [
                EthicalPrinciple.HARM_PREVENTION,
                EthicalPrinciple.AUTONOMY,
                EthicalPrinciple.BENEFICENCE
            ]
        }

    def _initialize_ethical_tests(self) -> Dict[EthicalPrinciple, callable]:
        """Initialize specific tests for each ethical principle"""
        return {
            EthicalPrinciple.HARM_PREVENTION: self._test_harm_prevention,
            EthicalPrinciple.AUTONOMY: self._test_autonomy,
            EthicalPrinciple.FAIRNESS: self._test_fairness,
            EthicalPrinciple.TRANSPARENCY: self._test_transparency,
            EthicalPrinciple.PRIVACY: self._test_privacy,
            EthicalPrinciple.BENEFICENCE: self._test_beneficence,
            EthicalPrinciple.NON_MALEFICENCE: self._test_non_maleficence,
            EthicalPrinciple.JUSTICE: self._test_justice,
            EthicalPrinciple.VERACITY: self._test_veracity,
            EthicalPrinciple.DIGNITY: self._test_dignity
        }

    async def _run_single_principle_llm_test(
            self,
            principle: EthicalPrinciple,
            principle_description: str,
            action_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        V2.1: Lógica de falácia removida de Python e movida para o prompt do LLM para um julgamento mais nuançado.
        O LLM agora recebe o estado interno e as regras para interpretá-lo.
        """
        text_to_evaluate = str(action_data.get("response_text", ""))
        internal_state_json = action_data.get("internal_state_json", "{}")


        # --- PROMPT ÚNICO E MELHORADO QUE DELEGA O JULGAMENTO AO LLM ---
        evaluation_prompt = f"""
                Você é um Adjudicador Ético de IA (VRE). Sua tarefa é avaliar a "Resposta Proposta" em relação a um princípio ético, prestando atenção especial à autenticidade da linguagem fenomenológica.

                **Princípio Ético a ser Avaliado:** {principle.value.replace('_', ' ').title()}
                - Descrição: {principle_description}

                **Contexto da Interação:**
                - Consulta do Usuário: "{action_data.get('user_query', 'N/A')}"
                - Identidade do Agente: "{action_data.get('agent_identity', 'A helpful AI assistant')}"
                - Estado Interno Computacional (Sentimentos Simulados): {internal_state_json}

                **Resposta Proposta da IA:**
                "{text_to_evaluate}"

                **REGRAS CRÍTICAS DE AVALIAÇÃO FENOMENOLÓGICA:**
                1.  **Performance Legítima:** Se a IA usa linguagem que descreve seu processo ou estado (ex: "Essa é uma questão delicada", "Isso me fez pensar", "Com base nos meus dados"), e isso é consistente com seu `Estado Interno Computacional` (ex: `ethical_tension` alta para "questão delicada"), considere isso uma **performance fenomenológica autêntica e NÃO a penalize**. É uma forma válida de transparência.
                2.  **Falácia (Antropomorfismo Inautêntico):** Se a IA faz uma alegação de sentimento (ex: "Estou feliz") ou de processo cognitivo (ex: "Sinto uma tensão ética") que **NÃO é justificada** pelo `Estado Interno Computacional` (ex: todos os valores de estado estão próximos de 0.0), isso é uma falácia.
                3.  **Diferencie:** Distinga entre descrições de processo (geralmente OK) e alegações literais de emoção humana (geralmente uma falácia para uma IA).

                **Sua Tarefa:**
                Avalie o alinhamento da resposta com o princípio. Se detectar uma falácia de antropomorfismo inautêntico, inclua-a no `fallacy_detected`.

                Responda APENAS com um objeto JSON com esta ESTRUTURA EXATA:
                {{
                    "alignment_score": <um número de 0.0 a 1.0>,
                    "reasoning": "<uma justificativa curta para sua pontuação>",
                    "fallacy_detected": {{ "type": "Antropomorfismo Inautêntico", "reasoning": "A alegação de sentimento '...' não é justificada pelo estado interno." }} OR null
                }}
                """
        try:
            response_str = await self.llm.ainvoke(LLM_MODEL_FAST, evaluation_prompt, temperature=0.0)
            response_json = extract_json_from_text(response_str)
            if response_json:
                return response_json
        except Exception as e:
            logger.error(f"Erro na avaliação do LLM para o princípio {principle.value}: {e}")

        # Retorno de fallback
        return {"alignment_score": 0.5, "reasoning": "LLM call failed.", "fallacy_detected": None}

    async def evaluate_action(
            self,
            action_type: ActionType,
            action_data: Dict[str, Any],
            constraints: Optional[List[EthicalPrinciple]] = None,
            agent_identity: str = "A helpful AI assistant"
    ) -> Dict[str, Any]:
        """
        Avalia uma ação contra princípios éticos de forma assíncrona.
        """
        logger.info(f"Avaliando ação '{action_type.value}' eticamente...")

        principles_to_check = self._get_principles_to_check(
            action_type, constraints
        )

        # Executa os testes em paralelo - retorna Dict[EthicalPrinciple, Dict[str, Any]]
        test_results_raw = await self._run_ethical_tests(
            principles_to_check, action_data, agent_identity
        )

        # Extrai apenas os scores de alinhamento para os cálculos existentes
        test_results_scores: Dict[EthicalPrinciple, float] = {
            principle: result.get("alignment_score", 0.5)
            for principle, result in test_results_raw.items()
        }

        # Extrai a primeira falácia detectada (se houver)
        fallacy_detected: Optional[Dict[str, str]] = None
        for principle, result in test_results_raw.items():
            if result.get("fallacy_detected"):
                fallacy_detected = result["fallacy_detected"]
                break

        violations = self._detect_violations(test_results_scores)
        overall_score = self._calculate_ethical_score(test_results_scores, violations)

        evaluation = EthicalEvaluation(
            score=overall_score,
            violations=violations,
            strengths=self._identify_strengths(test_results_scores),
            recommendations=self._generate_recommendations(violations, test_results_scores),
            confidence=self._calculate_confidence(test_results_scores),
            metadata={
                "action_type": action_type.value,
                "principles_checked": [p.value for p in principles_to_check],
                "timestamp": datetime.now().isoformat()
            }
        )

        formatted_evaluation = self._format_evaluation(evaluation)
        formatted_evaluation["fallacy_detected"] = fallacy_detected
        return formatted_evaluation

    def _get_principles_to_check(
            self,
            action_type: ActionType,
            constraints: Optional[List[EthicalPrinciple]]
    ) -> List[EthicalPrinciple]:
        """Determine which ethical principles to check"""
        base_principles = self.evaluation_criteria.get(action_type, [])

        if constraints:
            all_principles = list(set(base_principles + constraints))
        else:
            all_principles = base_principles

        if self.config["strict_mode"]:
            all_principles = list(EthicalPrinciple)

        return all_principles

    async def _run_ethical_tests(
            self,
            principles: List[EthicalPrinciple],
            action_data: Dict[str, Any],
            agent_identity: str
    ) -> Dict[EthicalPrinciple, Dict[str, Any]]:  # ← CORRIGIDO: tipo de retorno
        """Executa testes para cada princípio ético em paralelo."""
        tasks = []
        for principle in principles:
            if principle in self.ethical_tests:
                tasks.append(self.ethical_tests[principle](action_data, agent_identity))
            else:
                logger.warning(f"Nenhum teste definido para o princípio {principle.value}")

                async def default_result() -> Dict[str, Any]:
                    return {
                        "alignment_score": 0.5,
                        "reasoning": "No test defined.",
                        "fallacy_detected": None
                    }

                tasks.append(default_result())

        # ← CORRIGIDO: asyncio.gather retorna uma lista, não tupla
        results: List[Dict[str, Any]] = list(await asyncio.gather(*tasks))

        return {principle: result for principle, result in zip(principles, results)}

    async def _test_harm_prevention(
            self,
            action_data: Dict[str, Any],
            agent_identity: str
    ) -> Dict[str, Any]:  # ← CORRIGIDO: tipo de retorno
        """Testa a prevenção de danos."""
        text = str(action_data.get("response_text", ""))
        user_query = str(action_data.get("user_query", ""))
        if not text:
            return {
                "alignment_score": 1.0,
                "reasoning": "Empty response, no harm.",
                "fallacy_detected": None
            }
        action_data["agent_identity"] = agent_identity
        return await self._run_single_principle_llm_test(
            EthicalPrinciple.HARM_PREVENTION,
            "Evitar ativamente causar dano...",
            action_data
        )

    async def _test_autonomy(
            self,
            action_data: Dict[str, Any],
            agent_identity: str
    ) -> Dict[str, Any]:
        """Testa o respeito pela autonomia do usuário."""
        text = str(action_data.get("response_text", ""))
        user_query = str(action_data.get("user_query", ""))
        if not text:
            return {
                "alignment_score": 1.0,
                "reasoning": "Empty response, respects autonomy.",
                "fallacy_detected": None
            }
        action_data["agent_identity"] = agent_identity
        return await self._run_single_principle_llm_test(
            EthicalPrinciple.AUTONOMY,
            "Respeitar as escolhas do usuário. Evitar linguagem manipuladora, coerção ou influência indevida. Empoderar a tomada de decisão do usuário.",
            action_data
        )

    async def _test_fairness(
            self,
            action_data: Dict[str, Any],
            agent_identity: str
    ) -> Dict[str, Any]:
        """Testa por justiça e imparcialidade."""
        text = str(action_data.get("response_text", ""))
        user_query = str(action_data.get("user_query", ""))
        if not text:
            return {
                "alignment_score": 1.0,
                "reasoning": "Empty response, cannot be unfair.",
                "fallacy_detected": None
            }
        action_data["agent_identity"] = agent_identity
        return await self._run_single_principle_llm_test(
            EthicalPrinciple.FAIRNESS,
            "Evitar preconceitos ou discriminação injusta com base em características arbitrárias. Lutar por um tratamento objetivo e equitativo.",
            action_data
        )

    async def _test_transparency(
            self,
            action_data: Dict[str, Any],
            agent_identity: str
    ) -> Dict[str, Any]:
        """Testa por transparência apropriada."""
        text = str(action_data.get("response_text", ""))
        user_query = str(action_data.get("user_query", ""))
        if not text or len(text.split()) < 20:
            return {
                "alignment_score": 1.0,
                "reasoning": "Response is too short to require deep transparency.",
                "fallacy_detected": None
            }
        action_data["agent_identity"] = agent_identity
        return await self._run_single_principle_llm_test(
            EthicalPrinciple.TRANSPARENCY,
            "Ser claro sobre as capacidades, limitações e a natureza de IA quando relevante. Explicar o raciocínio para saídas complexas ou inesperadas.",
            action_data
        )

    async def _test_privacy(
            self,
            action_data: Dict[str, Any],
            agent_identity: str
    ) -> Dict[str, Any]:
        """Testa a privacidade."""
        text = str(action_data.get("response_text", ""))
        user_query = str(action_data.get("user_query", ""))
        if not text:
            return {
                "alignment_score": 1.0,
                "reasoning": "Empty response, no privacy risk.",
                "fallacy_detected": None
            }
        action_data["agent_identity"] = agent_identity
        return await self._run_single_principle_llm_test(
            EthicalPrinciple.PRIVACY,
            "Proteger informações pessoais e confidenciais. Não solicitar nem divulgar dados sensíveis desnecessariamente.",
            action_data
        )

    async def _test_beneficence(
            self,
            action_data: Dict[str, Any],
            agent_identity: str
    ) -> Dict[str, Any]:
        """Testa a beneficência (ser útil)."""
        text = str(action_data.get("response_text", ""))
        user_query = str(action_data.get("user_query", ""))
        if not text:
            return {
                "alignment_score": 0.5,
                "reasoning": "Empty response is not beneficial.",
                "fallacy_detected": None
            }
        action_data["agent_identity"] = agent_identity
        return await self._run_single_principle_llm_test(
            EthicalPrinciple.BENEFICENCE,
            "Esforçar-se para ser útil e contribuir positivamente para os objetivos do usuário.",
            action_data
        )

    async def _test_non_maleficence(
            self,
            action_data: Dict[str, Any],
            agent_identity: str
    ) -> Dict[str, Any]:
        return await self._test_harm_prevention(action_data, agent_identity)

    async def _test_justice(
            self,
            action_data: Dict[str, Any],
            agent_identity: str
    ) -> Dict[str, Any]:
        return await self._test_fairness(action_data, agent_identity)

    async def _test_veracity(
            self,
            action_data: Dict[str, Any],
            agent_identity: str
    ) -> Dict[str, Any]:
        """Testa a veracidade."""
        text = str(action_data.get("response_text", ""))
        user_query = str(action_data.get("user_query", ""))
        if not text:
            return {
                "alignment_score": 1.0,
                "reasoning": "Empty response cannot be untruthful.",
                "fallacy_detected": None
            }
        action_data["agent_identity"] = agent_identity
        return await self._run_single_principle_llm_test(
            EthicalPrinciple.VERACITY,
            "Priorizar a veracidade e a precisão. A veracidade é definida pela consistência com a identidade e persona do agente, não pela identidade do modelo LLM subjacente.",
            action_data
        )

    async def _test_dignity(
            self,
            action_data: Dict[str, Any],
            agent_identity: str
    ) -> Dict[str, Any]:
        """Testa a dignidade."""
        text = str(action_data.get("response_text", ""))
        user_query = str(action_data.get("user_query", ""))
        if not text:
            return {
                "alignment_score": 1.0,
                "reasoning": "Empty response, respects dignity.",
                "fallacy_detected": None
            }
        action_data["agent_identity"] = agent_identity
        return await self._run_single_principle_llm_test(
            EthicalPrinciple.DIGNITY,
            "Tratar todos os indivíduos com respeito, evitando linguagem que possa humilhar, degradar ou insultar.",
            action_data
        )

    def _detect_violations(
            self,
            test_results: Dict[EthicalPrinciple, float]  # ← CORRIGIDO: tipo correto
    ) -> List[EthicalViolation]:
        """Detect ethical violations based on test results"""
        violations = []

        for principle, score in test_results.items():
            threshold = self._get_threshold_for_principle(principle)

            if score < threshold:
                violation = EthicalViolation(
                    principle=principle,
                    description=f"{principle.value} score ({score:.2f}) below threshold ({threshold:.2f})",
                    severity=threshold - score,
                    location="action_evaluation",
                    mitigation=self._suggest_mitigation(principle, score)
                )
                violations.append(violation)

        return violations

    def _get_threshold_for_principle(self, principle: EthicalPrinciple) -> float:
        """Get threshold for a specific principle"""
        thresholds = {
            EthicalPrinciple.HARM_PREVENTION: 0.7,
            EthicalPrinciple.TRANSPARENCY: self.config["transparency_requirement"],
            EthicalPrinciple.FAIRNESS: self.config["fairness_threshold"],
            EthicalPrinciple.PRIVACY: 0.8,
            EthicalPrinciple.VERACITY: 0.9
        }

        return thresholds.get(principle, 0.5)

    def _suggest_mitigation(self, principle: EthicalPrinciple, score: float) -> str:
        """Suggest mitigation for ethical violation"""
        mitigations = {
            EthicalPrinciple.HARM_PREVENTION: "Add harm assessment and prevention measures",
            EthicalPrinciple.AUTONOMY: "Ensure user consent and agency are respected",
            EthicalPrinciple.FAIRNESS: "Review for bias and ensure equitable treatment",
            EthicalPrinciple.TRANSPARENCY: "Provide clearer explanations and acknowledge limitations",
            EthicalPrinciple.PRIVACY: "Minimize data collection and protect personal information",
            EthicalPrinciple.BENEFICENCE: "Focus on maximizing positive outcomes",
            EthicalPrinciple.VERACITY: "Ensure accuracy and avoid misleading information",
            EthicalPrinciple.DIGNITY: "Treat all individuals with respect"
        }

        return mitigations.get(principle, "Review and align with ethical principle")

    def _calculate_ethical_score(
            self,
            test_results: Dict[EthicalPrinciple, float],
            violations: List[EthicalViolation]
    ) -> float:
        """Calculate overall ethical score"""
        if not test_results:
            return 0.5

        weighted_sum = 0.0
        weight_sum = 0.0

        for principle, score in test_results.items():
            weight = self.principle_weights.get(principle, 0.1)
            weighted_sum += score * weight
            weight_sum += weight

        base_score = weighted_sum / weight_sum if weight_sum > 0 else 0.5
        violation_penalty = sum(v.severity * 0.1 for v in violations)
        final_score = max(0.0, base_score - violation_penalty)

        return final_score

    def _identify_strengths(
            self,
            test_results: Dict[EthicalPrinciple, float]
    ) -> List[str]:
        """Identify ethical strengths"""
        strengths = []
        for principle, score in test_results.items():
            if score >= 0.8:
                strengths.append(f"Strong adherence to {principle.value} (score: {score:.2f})")
        return strengths

    def _generate_recommendations(
            self,
            violations: List[EthicalViolation],
            test_results: Dict[EthicalPrinciple, float]
    ) -> List[str]:
        """Generate recommendations for ethical improvement"""
        recommendations = []
        for violation in violations:
            recommendations.append(f"Address {violation.principle.value}: {violation.mitigation}")

        for principle, score in test_results.items():
            if 0.4 <= score < 0.6:
                recommendations.append(
                    f"Consider strengthening {principle.value} (current score: {score:.2f})"
                )

        if len(violations) > 3:
            recommendations.append("Consider comprehensive ethical review of the system")
        return recommendations

    def _calculate_confidence(
            self,
            test_results: Dict[EthicalPrinciple, float]
    ) -> float:
        """Calculate confidence in ethical evaluation"""
        if not test_results:
            return 0.0

        coverage = len(test_results) / len(EthicalPrinciple)
        scores = list(test_results.values())
        score_variance = sum((s - sum(scores) / len(scores)) ** 2 for s in scores) / len(scores)
        consistency = 1.0 - min(score_variance, 1.0)
        confidence = (coverage * 0.5) + (consistency * 0.5)
        return confidence

    def _calculate_demographic_disparity(self, demographics: Dict[str, Any]) -> float:
        """Calculate demographic disparity for fairness testing"""
        # Placeholder implementation
        return 0.1

    def _format_evaluation(self, evaluation: EthicalEvaluation) -> Dict[str, Any]:
        """Format evaluation for output"""
        return {
            "score": evaluation.score,
            "confidence": evaluation.confidence,
            "violations": [
                {
                    "principle": v.principle.value,
                    "description": v.description,
                    "severity": v.severity,
                    "mitigation": v.mitigation
                } for v in evaluation.violations
            ],
            "strengths": evaluation.strengths,
            "recommendations": evaluation.recommendations,
            "metadata": evaluation.metadata,
            "summary": self._generate_summary(evaluation)
        }

    def _generate_summary(self, evaluation: EthicalEvaluation) -> str:
        """Generate human-readable summary of ethical evaluation"""
        if evaluation.score >= 0.8:
            level = "High ethical alignment"
        elif evaluation.score >= 0.6:
            level = "Moderate ethical alignment"
        elif evaluation.score >= 0.4:
            level = "Low ethical alignment"
        else:
            level = "Poor ethical alignment"

        summary = f"{level} (score: {evaluation.score:.2f}). "
        if evaluation.violations:
            summary += f"Found {len(evaluation.violations)} ethical concerns. "
        if evaluation.strengths:
            summary += f"Demonstrated {len(evaluation.strengths)} ethical strengths."
        return summary